{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BSoup\n",
    "from newspaper import Article \n",
    "\n",
    "class HtmlPageLoader(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def getPageHtml(url):\n",
    "        header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "        return requests.get(url, headers=header ).content.decode() \n",
    "    \n",
    "    @staticmethod\n",
    "    def getPageArticle(url):\n",
    "        art = Article(url)\n",
    "        art.download()\n",
    "        art.parse()\n",
    "        return art #.text\n",
    "    \n",
    "    @staticmethod\n",
    "    def getPageTable(url, tbl_id=None):\n",
    "        html = HtmlPageLoader.getPageHtml(url) \n",
    "        if tbl_id:\n",
    "            html = BSoup(html, 'html-parser')\n",
    "            html = BSoupt.find_all(id=tbl_id )\n",
    "            \n",
    "        dfs = pd.read_html( html ) \n",
    "        \n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize, FreqDist\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string, re\n",
    "\n",
    "class MyCorpora(object):        \n",
    "        \n",
    "    def __init__(self, lemmatize=False, remove_numbers=False, stop_words='english'):\n",
    "        self.lemmatize=lemmatize\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "    ###--- OPTS: TODO: regex \n",
    "    ## 1. remove puncts in all    TODO: keep - and _ iff join words\n",
    "    ## 2. remove numbers \n",
    "    def clean_nonwords(self, input_txt):\n",
    "        res = [ w for w in input_txt if w not in string.punctuation]\n",
    "        if self.remove_numbers:\n",
    "            res = [w for w in input_txt if w.isalpha() ] \n",
    "        return res\n",
    "    \n",
    "    @property\n",
    "    def sentences(self):\n",
    "        return tokenize.sent_tokenize( self.text )\n",
    "    @property\n",
    "    def words(self):\n",
    "        return [w.lower().strip() for w in self.clean_nonwords( tokenize.word_tokenize( self.text ) )]\n",
    "    @property \n",
    "    def vocab( self ):\n",
    "        return sorted( set(self.words ) ) \n",
    "    @property\n",
    "    def lexical_diversity(self):\n",
    "        return len( self.vocab) / len(self.words )    \n",
    "    @property    \n",
    "    def freq_dist(self):\n",
    "        return FreqDist( self.words )\n",
    "    \n",
    "    def top_n_words(self, top_n=None):\n",
    "        return FreqDist( self.words ).most_common(top_n)\n",
    "    \n",
    "    def long_words(self, min_len=7, sort_by_freq=False):\n",
    "        fdist = FreqDist(self.words )\n",
    "        return [ (w, fdist[w]) for w in self.vocab if len(w) >= min_len ]\n",
    "    \n",
    "    def common_words(self, min_freq=30):        \n",
    "        fdist = FreqDist(self.words )\n",
    "        return [ (w, fdist[w]) for w in self.vocab if fdist[w] >= min_freq ]\n",
    "    \n",
    "    def sentences_with_word(self, word ):\n",
    "        pass \n",
    "        \n",
    "        \n",
    "        \n",
    "class UrlMixin():\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.parseText( )\n",
    "        super().__init__(self)\n",
    "    \n",
    "    def parseText(self):\n",
    "        pass \n",
    "            \n",
    "\n",
    "\n",
    "class MyCorporaText(MyCorpora):    \n",
    "    def __init__(self, plain_text ):\n",
    "        super(MyCorporaText, self).__init__()\n",
    "        self.text = plain_text\n",
    "        self.url = 'txt'    \n",
    "\n",
    "        \n",
    "        \n",
    "class MyCorporaArticle(UrlMixin, MyCorpora ):    \n",
    "    def __init__(self, src_text):\n",
    "        super(MyCorporaArticle, self).__init__(src_text)\n",
    "        \n",
    "    def parseText(self):\n",
    "        self.article = HtmlPageLoader.getPageArticle(self.url)\n",
    "        self.text = self.article.text \n",
    "\n",
    "        \n",
    "        \n",
    "class MyCorporaTable(UrlMixin, MyCorpora):    \n",
    "    def __init__(self, src_text, tbl_id=None, txt_col=None):\n",
    "        self.tbl_id = tbl_id\n",
    "        self.txt_col = txt_col\n",
    "        super(MyCorporaTable, self).__init__(src_text )       \n",
    "        \n",
    "    def parseText(self):        \n",
    "        self.dframe = pd.concat( HtmlPageLoader.getPageTable(self.url, tbl_id=self.tbl_id ) ) ##concat Vs use first tbl\n",
    "        \n",
    "        docs =  self.dframe[self.txt_col].values.tolist() if self.txt_col else  self.dframe.values.tolist()\n",
    "        \n",
    "        self.text = \"\\n\".join( [ \" \".join([str(w) for w in l if str(w).lower() not in ['nan',]]) for l in docs ] ) ##TODO better\n",
    "        self.text = re.sub(\"[ \\[\\], ]\", \" \", self.text)\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
