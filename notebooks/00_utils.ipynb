{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as BSoup\n",
    "from newspaper import Article \n",
    "\n",
    "class HtmlPageLoader(object):\n",
    "    \n",
    "    @staticmethod\n",
    "    def getPageHtml(url):\n",
    "        header = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.102 Safari/537.36'}\n",
    "        return requests.get(url, headers=header ).content.decode() \n",
    "    \n",
    "    @staticmethod\n",
    "    def getPageArticle(url):\n",
    "        art = Article(url)\n",
    "        art.download()\n",
    "        art.parse()\n",
    "        return art #.text\n",
    "    \n",
    "    @staticmethod\n",
    "    def getPageTable(url, tbl_id=None):\n",
    "        html = HtmlPageLoader.getPageHtml(url) \n",
    "        if tbl_id:\n",
    "            html = BSoup(html, 'html-parser')\n",
    "            html = BSoupt.find_all(id=tbl_id )\n",
    "            \n",
    "        dfs = pd.read_html( html ) \n",
    "        \n",
    "        return dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize, FreqDist, pos_tag, pos_tag_sents\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords \n",
    "import string, re\n",
    "\n",
    "##TODO: compare with nltk sentiment analyzer\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer as Sentimentor \n",
    "\n",
    "class MyCorpora(object):        \n",
    "        \n",
    "    def __init__(self, lemmatize=False, remove_numbers=False, stop_words='english'):\n",
    "        self.lemmatize=lemmatize\n",
    "        self.remove_numbers = remove_numbers\n",
    "        self.stop_words = stop_words\n",
    "        \n",
    "    ###--- OPTS: TODO: regex \n",
    "    ## 1. remove puncts in all    TODO: keep - and _ iff join words\n",
    "    ## 2. remove numbers \n",
    "    def clean_nonwords(self, input_txt):\n",
    "        res = [ w for w in input_txt if w not in string.punctuation]\n",
    "        if self.remove_numbers:\n",
    "            res = [w for w in res if w.isalpha() ] \n",
    "        if self.stop_words:\n",
    "            res = [w for w in res if w not in stopwords.words('english') ]             \n",
    "        return res\n",
    "    \n",
    "    @property\n",
    "    def sentences(self):\n",
    "        return tokenize.sent_tokenize( self.text )\n",
    "    @property\n",
    "    def words(self):\n",
    "        return [w.lower().strip() for w in self.clean_nonwords( tokenize.word_tokenize( self.text ) )]\n",
    "    \n",
    "    @property\n",
    "    def lemmas(self, remove_stops=False):\n",
    "        lemm = WordNetLemmatizer()\n",
    "        return [ wl for wl in [lemm.lemmatize( w ) for w in self.words ] if len(wl) > 2 ]\n",
    "    @property \n",
    "    def vocab( self ):\n",
    "        return sorted( set(self.words ) ) \n",
    "    @property\n",
    "    def lexical_diversity(self):\n",
    "        return len( self.vocab) / len(self.words )   \n",
    "    @property\n",
    "    def mean_word_length(self):\n",
    "        return np.array( [len( w ) for w in self.vocab]).mean()   \n",
    "    @property\n",
    "    def mean_sentence_length(self):\n",
    "        return np.array( [len( s ) for s in self.sentences]).mean()    \n",
    "    @property    \n",
    "    def freq_dist(self):\n",
    "        return FreqDist( self.words )\n",
    "    \n",
    "    def top_n_words(self, top_n=None):\n",
    "        return FreqDist( self.words ).most_common(top_n)\n",
    "    \n",
    "    def long_words(self, min_len=7, sort_by_freq=False):\n",
    "        fdist = FreqDist(self.words )\n",
    "        return [ (w, fdist[w]) for w in self.vocab if len(w) >= min_len ]\n",
    "    \n",
    "    def common_words(self, min_freq=30):        \n",
    "        fdist = FreqDist(self.words )\n",
    "        return [ (w, fdist[w]) for w in self.vocab if fdist[w] >= min_freq ]\n",
    "    \n",
    "    def sentences_with_word(self, word ):\n",
    "        pass \n",
    "    \n",
    "    # ref: https://github.com/cjhutto/vaderSentiment \n",
    "    # ref: https://medium.com/analytics-vidhya/simplifying-social-media-sentiment-analysis-using-vader-in-python-f9e6ec6fc52f\n",
    "    @property \n",
    "    def sentiment_score(self):\n",
    "        sz = Sentimentor()\n",
    "        return sz.polarity_scores(self.text )\n",
    "    ## TODO: is naive at present \n",
    "    def pos_tag_stats(self):\n",
    "        self.pos_stats = {\n",
    "            'i_counts': 0,\n",
    "            'axn_ratio': 0,\n",
    "            'adv_ratio':0,\n",
    "        }\n",
    "        \n",
    "        res = []\n",
    "        for sent in self.sentences:\n",
    "            tokenz = [ w.lower() for w in tokenize.word_tokenize(sent)]\n",
    "            slen = len(tokenz)\n",
    "            tagz = pos_tag(  tokenz ) \n",
    "            iz = np.array( [ 1 for w, t in tagz if t == 'NN' and w == 'i' ] ).sum()\n",
    "            axnz = np.array([ 1 for w, t in tagz if t == 'VBD' ]).sum() / slen\n",
    "            advz = np.array([ 1 for w, t in tagz if t == 'RB' ]).sum() / slen \n",
    "            res.append( (iz, axnz, advz) )\n",
    "            \n",
    "        self.pos_stats['i_counts'] = np.array([r[0] for r in res]).mean()\n",
    "        self.pos_stats['axn_ratio'] = np.array([r[1] for r in res]).mean()\n",
    "        self.pos_stats['adv_ratio'] = np.array([r[2] for r in res]).mean()\n",
    "        \n",
    "        return self.pos_stats\n",
    "        \n",
    "class UrlMixin():\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        self.parseText( )\n",
    "        super().__init__(self)\n",
    "    \n",
    "    def parseText(self):\n",
    "        pass \n",
    "            \n",
    "\n",
    "\n",
    "class MyCorporaText(MyCorpora):    \n",
    "    def __init__(self, plain_text, lemmatize=False, remove_numbers=False, stop_words='english' ):\n",
    "        super(MyCorporaText, self).__init__(lemmatize=lemmatize, remove_numbers=remove_numbers, stop_words='english')\n",
    "        self.text = plain_text\n",
    "        self.url = 'txt'    \n",
    "\n",
    "        \n",
    "        \n",
    "class MyCorporaArticle(UrlMixin, MyCorpora ):    \n",
    "    def __init__(self, src_text):\n",
    "        super(MyCorporaArticle, self).__init__(src_text)\n",
    "        \n",
    "    def parseText(self):\n",
    "        self.article = HtmlPageLoader.getPageArticle(self.url)\n",
    "        self.text = self.article.text \n",
    "\n",
    "        \n",
    "        \n",
    "class MyCorporaTable(UrlMixin, MyCorpora):    \n",
    "    def __init__(self, src_text, tbl_id=None, txt_col=None):\n",
    "        self.tbl_id = tbl_id\n",
    "        self.txt_col = txt_col\n",
    "        super(MyCorporaTable, self).__init__(src_text )       \n",
    "        \n",
    "    def parseText(self):        \n",
    "        self.dframe = pd.concat( HtmlPageLoader.getPageTable(self.url, tbl_id=self.tbl_id ) ) ##concat Vs use first tbl\n",
    "        \n",
    "        docs =  self.dframe[self.txt_col].values.tolist() if self.txt_col else  self.dframe.values.tolist()\n",
    "        \n",
    "        self.text = \"\\n\".join( [ \" \".join([str(w) for w in l if str(w).lower() not in ['nan',]]) for l in docs ] ) ##TODO better\n",
    "        self.text = re.sub(\"[ \\[\\], ]\", \" \", self.text)\n",
    "  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
